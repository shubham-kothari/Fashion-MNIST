{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FashionMNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-kothari/Fashion-MNIST/blob/master/FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tIQS67tsNwT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lc-4E_OdPXZU",
        "colab_type": "code",
        "outputId": "688e9e98-6da8-479b-9a49-a920574199d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the fashion-mnist pre-shuffled train data and test data\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hGwPFImnP3ks",
        "colab_type": "code",
        "outputId": "59dde97d-032f-4173-e0f1-300d3d47fefe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# Print the number of training and test datasets\n",
        "print(x_train.shape[0], 'train data set')\n",
        "print(x_test.shape[0], 'test data set')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train data set\n",
            "10000 test data set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cwmLipAsPzVz",
        "colab_type": "code",
        "outputId": "870821c9-a984-44ed-9e37-411115f386a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Print training set shape - note there are 60,000 training data of image size of 28x28, 60,000 train labels)\n",
        "\n",
        "print(\"X Training Data shape:\", x_train.shape, \"Y train shape:\", y_train.shape)\n",
        "print(\"X Training Data shape:\", x_test.shape, \"Y train shape:\", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Training Data shape: (60000, 28, 28) Y train shape: (60000,)\n",
            "X Training Data shape: (10000, 28, 28) Y train shape: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2E2X5QyXNzVj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the text labels\n",
        "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
        "                        \"Trouser\",      # index 1\n",
        "                        \"Pullover\",     # index 2 \n",
        "                        \"Dress\",        # index 3 \n",
        "                        \"Coat\",         # index 4\n",
        "                        \"Sandal\",       # index 5\n",
        "                        \"Shirt\",        # index 6 \n",
        "                        \"Sneaker\",      # index 7 \n",
        "                        \"Bag\",          # index 8 \n",
        "                        \"Ankle boot\"]   # index 9\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "biHScQQLQqDV",
        "colab_type": "code",
        "outputId": "f2a12df7-3ab2-496a-ad21-5b34a038f8e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "cell_type": "code",
      "source": [
        "label = y_train[59999]\n",
        "print(fashion_mnist_labels[label])\n",
        "plt.imshow(x_train[59999])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sandal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f907b05f4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WGH56Yj_QXjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOm6KCdhSGCn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
        "(x_train, x_valid) = x_train[5900:], x_train[:5900] \n",
        "(y_train, y_valid) = y_train[5900:], y_train[:5900]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_y74pR_8SMuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtC7qDhjSU63",
        "colab_type": "code",
        "outputId": "333e0887-bd1b-4622-b457-74ce952ae8b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(x_train.shape,x_valid.shape,x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54100, 28, 28) (5900, 28, 28) (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tfAmpNf1SJrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape input data from (28, 28) to (28, 28, 1)\n",
        "\n",
        "x_train = x_train.reshape(54100,28,28,1)\n",
        "x_valid = x_valid.reshape(5900,28,28, 1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ZuaWSS9SQzj",
        "colab_type": "code",
        "outputId": "e6de8d39-4c12-4d9c-dcb5-60f1bfbf2016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Print training set shape\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the number of training, validation, and test datasets\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_valid.shape[0], 'validation set')\n",
        "print(x_test.shape[0], 'test set')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (54100, 28, 28, 1) y_train shape: (54100, 10)\n",
            "54100 train set\n",
            "5900 validation set\n",
            "10000 test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lMbtYDK6TZxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Must define the input shape in the first layer of the neural network\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=3))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=3))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Take a look at the model summary\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWeShHxuXZRk",
        "colab_type": "code",
        "outputId": "d0775b40-5a65-4f9f-eb86-b0331d3cb57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test set\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "# Print test accuracy\n",
        "print('\\n', 'Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Test accuracy: 0.0996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xkGW4FsKZnWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2557
        },
        "outputId": "fa918c08-47cb-4a55-acac-76cd54ec3524"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train,y_train, batch_size=69, epochs=99, validation_data=(x_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54100 samples, validate on 5900 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/99\n",
            "54100/54100 [==============================] - 8s 148us/sample - loss: 1.1042 - acc: 0.5818 - val_loss: 0.5389 - val_acc: 0.7917\n",
            "Epoch 2/99\n",
            "54100/54100 [==============================] - 8s 140us/sample - loss: 0.6698 - acc: 0.7483 - val_loss: 0.4594 - val_acc: 0.8175\n",
            "Epoch 3/99\n",
            "54100/54100 [==============================] - 7s 138us/sample - loss: 0.5969 - acc: 0.7795 - val_loss: 0.4207 - val_acc: 0.8476\n",
            "Epoch 4/99\n",
            "54100/54100 [==============================] - 7s 137us/sample - loss: 0.5484 - acc: 0.7995 - val_loss: 0.3981 - val_acc: 0.8510\n",
            "Epoch 5/99\n",
            "54100/54100 [==============================] - 7s 138us/sample - loss: 0.5126 - acc: 0.8162 - val_loss: 0.3635 - val_acc: 0.8620\n",
            "Epoch 6/99\n",
            "54100/54100 [==============================] - 7s 139us/sample - loss: 0.4926 - acc: 0.8256 - val_loss: 0.3509 - val_acc: 0.8632\n",
            "Epoch 7/99\n",
            "54100/54100 [==============================] - 7s 138us/sample - loss: 0.4752 - acc: 0.8331 - val_loss: 0.3300 - val_acc: 0.8822\n",
            "Epoch 8/99\n",
            "54100/54100 [==============================] - 7s 138us/sample - loss: 0.4608 - acc: 0.8383 - val_loss: 0.3188 - val_acc: 0.8841\n",
            "Epoch 9/99\n",
            "54100/54100 [==============================] - 7s 137us/sample - loss: 0.4521 - acc: 0.8419 - val_loss: 0.3109 - val_acc: 0.8822\n",
            "Epoch 10/99\n",
            "54100/54100 [==============================] - 7s 137us/sample - loss: 0.4391 - acc: 0.8461 - val_loss: 0.3081 - val_acc: 0.8863\n",
            "Epoch 11/99\n",
            "54100/54100 [==============================] - 7s 138us/sample - loss: 0.4331 - acc: 0.8472 - val_loss: 0.2942 - val_acc: 0.8897\n",
            "Epoch 12/99\n",
            "54100/54100 [==============================] - 8s 151us/sample - loss: 0.4227 - acc: 0.8498 - val_loss: 0.2876 - val_acc: 0.8973\n",
            "Epoch 13/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.4167 - acc: 0.8513 - val_loss: 0.2835 - val_acc: 0.8936\n",
            "Epoch 14/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.4114 - acc: 0.8550 - val_loss: 0.2812 - val_acc: 0.8949\n",
            "Epoch 15/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.4074 - acc: 0.8574 - val_loss: 0.2786 - val_acc: 0.8976\n",
            "Epoch 16/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.4028 - acc: 0.8577 - val_loss: 0.2713 - val_acc: 0.8963\n",
            "Epoch 17/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.4048 - acc: 0.8580 - val_loss: 0.2722 - val_acc: 0.9003\n",
            "Epoch 18/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3994 - acc: 0.8591 - val_loss: 0.2804 - val_acc: 0.8939\n",
            "Epoch 19/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3952 - acc: 0.8604 - val_loss: 0.2643 - val_acc: 0.9046\n",
            "Epoch 20/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3908 - acc: 0.8633 - val_loss: 0.2744 - val_acc: 0.8939\n",
            "Epoch 21/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3905 - acc: 0.8643 - val_loss: 0.2648 - val_acc: 0.8997\n",
            "Epoch 22/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3857 - acc: 0.8649 - val_loss: 0.2591 - val_acc: 0.9059\n",
            "Epoch 23/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3820 - acc: 0.8653 - val_loss: 0.2601 - val_acc: 0.9017\n",
            "Epoch 24/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3805 - acc: 0.8653 - val_loss: 0.2562 - val_acc: 0.9019\n",
            "Epoch 25/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3806 - acc: 0.8667 - val_loss: 0.2661 - val_acc: 0.9007\n",
            "Epoch 26/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3820 - acc: 0.8655 - val_loss: 0.2587 - val_acc: 0.9080\n",
            "Epoch 27/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3763 - acc: 0.8669 - val_loss: 0.2560 - val_acc: 0.9046\n",
            "Epoch 28/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3749 - acc: 0.8675 - val_loss: 0.2590 - val_acc: 0.9010\n",
            "Epoch 29/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3764 - acc: 0.8667 - val_loss: 0.2625 - val_acc: 0.9010\n",
            "Epoch 30/99\n",
            "54100/54100 [==============================] - 8s 140us/sample - loss: 0.3745 - acc: 0.8685 - val_loss: 0.2524 - val_acc: 0.9049\n",
            "Epoch 31/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3715 - acc: 0.8678 - val_loss: 0.2545 - val_acc: 0.9036\n",
            "Epoch 32/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3747 - acc: 0.8674 - val_loss: 0.2882 - val_acc: 0.8907\n",
            "Epoch 33/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3711 - acc: 0.8678 - val_loss: 0.2485 - val_acc: 0.9083\n",
            "Epoch 34/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3703 - acc: 0.8695 - val_loss: 0.2490 - val_acc: 0.9058\n",
            "Epoch 35/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3711 - acc: 0.8697 - val_loss: 0.2516 - val_acc: 0.9068\n",
            "Epoch 36/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3647 - acc: 0.8701 - val_loss: 0.2484 - val_acc: 0.9068\n",
            "Epoch 37/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3639 - acc: 0.8720 - val_loss: 0.2541 - val_acc: 0.9076\n",
            "Epoch 38/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3609 - acc: 0.8706 - val_loss: 0.2440 - val_acc: 0.9071\n",
            "Epoch 39/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3643 - acc: 0.8710 - val_loss: 0.2503 - val_acc: 0.9078\n",
            "Epoch 40/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3633 - acc: 0.8708 - val_loss: 0.2490 - val_acc: 0.9097\n",
            "Epoch 41/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3614 - acc: 0.8735 - val_loss: 0.2560 - val_acc: 0.9031\n",
            "Epoch 42/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3586 - acc: 0.8727 - val_loss: 0.2520 - val_acc: 0.9025\n",
            "Epoch 43/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3599 - acc: 0.8724 - val_loss: 0.2476 - val_acc: 0.9114\n",
            "Epoch 44/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3633 - acc: 0.8713 - val_loss: 0.2460 - val_acc: 0.9069\n",
            "Epoch 45/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3600 - acc: 0.8729 - val_loss: 0.2456 - val_acc: 0.9086\n",
            "Epoch 46/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3571 - acc: 0.8740 - val_loss: 0.2435 - val_acc: 0.9085\n",
            "Epoch 47/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3585 - acc: 0.8752 - val_loss: 0.2536 - val_acc: 0.9042\n",
            "Epoch 48/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3562 - acc: 0.8750 - val_loss: 0.2602 - val_acc: 0.9020\n",
            "Epoch 49/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3547 - acc: 0.8748 - val_loss: 0.2421 - val_acc: 0.9093\n",
            "Epoch 50/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3594 - acc: 0.8746 - val_loss: 0.2418 - val_acc: 0.9093\n",
            "Epoch 51/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3547 - acc: 0.8747 - val_loss: 0.2446 - val_acc: 0.9117\n",
            "Epoch 52/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3544 - acc: 0.8745 - val_loss: 0.2454 - val_acc: 0.9102\n",
            "Epoch 53/99\n",
            "54100/54100 [==============================] - 8s 140us/sample - loss: 0.3512 - acc: 0.8752 - val_loss: 0.2502 - val_acc: 0.9058\n",
            "Epoch 54/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3501 - acc: 0.8745 - val_loss: 0.2451 - val_acc: 0.9039\n",
            "Epoch 55/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3569 - acc: 0.8748 - val_loss: 0.2524 - val_acc: 0.9061\n",
            "Epoch 56/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3499 - acc: 0.8757 - val_loss: 0.2429 - val_acc: 0.9127\n",
            "Epoch 57/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3494 - acc: 0.8762 - val_loss: 0.2433 - val_acc: 0.9083\n",
            "Epoch 58/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3521 - acc: 0.8764 - val_loss: 0.2453 - val_acc: 0.9097\n",
            "Epoch 59/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3564 - acc: 0.8738 - val_loss: 0.2493 - val_acc: 0.9081\n",
            "Epoch 60/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3481 - acc: 0.8784 - val_loss: 0.2507 - val_acc: 0.9098\n",
            "Epoch 61/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3438 - acc: 0.8791 - val_loss: 0.2471 - val_acc: 0.9047\n",
            "Epoch 62/99\n",
            "54100/54100 [==============================] - 8s 140us/sample - loss: 0.3479 - acc: 0.8783 - val_loss: 0.2395 - val_acc: 0.9124\n",
            "Epoch 63/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3467 - acc: 0.8799 - val_loss: 0.2396 - val_acc: 0.9127\n",
            "Epoch 64/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3471 - acc: 0.8769 - val_loss: 0.2415 - val_acc: 0.9105\n",
            "Epoch 65/99\n",
            "54100/54100 [==============================] - 8s 143us/sample - loss: 0.3469 - acc: 0.8775 - val_loss: 0.2382 - val_acc: 0.9132\n",
            "Epoch 66/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3481 - acc: 0.8773 - val_loss: 0.2382 - val_acc: 0.9137\n",
            "Epoch 67/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3448 - acc: 0.8774 - val_loss: 0.2408 - val_acc: 0.9108\n",
            "Epoch 68/99\n",
            "54100/54100 [==============================] - 8s 141us/sample - loss: 0.3467 - acc: 0.8787 - val_loss: 0.2371 - val_acc: 0.9092\n",
            "Epoch 69/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3397 - acc: 0.8799 - val_loss: 0.2508 - val_acc: 0.9034\n",
            "Epoch 70/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3446 - acc: 0.8789 - val_loss: 0.2518 - val_acc: 0.9073\n",
            "Epoch 71/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3429 - acc: 0.8791 - val_loss: 0.2478 - val_acc: 0.9059\n",
            "Epoch 72/99\n",
            "54100/54100 [==============================] - 8s 142us/sample - loss: 0.3489 - acc: 0.8774 - val_loss: 0.2496 - val_acc: 0.9088\n",
            "Epoch 73/99\n",
            "42159/54100 [======================>.......] - ETA: 1s - loss: 0.3441 - acc: 0.8786"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}